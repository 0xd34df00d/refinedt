% vim: set ft=tex:

%\documentclass{jfp}
%\usepackage{amsmath,amsfonts,amssymb}

\documentclass[a4paper]{article}
\usepackage{a4wide}
\usepackage{amsmath,amsfonts,amssymb,amsthm}

\tracinglostchars=2
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{hyperref,cleveref}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage[implicitLineBreakHack]{ottalt}
\usepackage{subcaption}
\usepackage{cite,url,babelbib}
\usepackage{todonotes}
\usepackage[english]{babel}

\nonstopmode

\addtolength{\topmargin}{-1.05in}
\addtolength{\textheight}{1.55in}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\crefname{lemma}{Lemma}{Lemmas}
\crefname{remark}{Remark}{Remarks}

\newcommand{\unit}{\textbf{u}}
\newcommand{\Unit}{\textbf{Unit}}
\newcommand{\unitc}{\hat{\textbf{u}}}
\newcommand{\Unitc}{\widehat{\textbf{Unit}}}
\newcommand{\Basec}{\hat B}
\newcommand{\Int}{\textbf{Int}}
\newcommand{\ctxok}{\text{~ok}}
\newcommand{\fresh}{\text{~fresh name}}
\newcommand{\evalsto}{\rightsquigarrow}
\newcommand{\eqrefl}{\text{eq-refl~}}
\newcommand{\secondDP}{\text{second-dp~}}

\newcommand\prooftermname[1]{\texttt{#1}}

\begin{document}

\newNTclass{nonterm}
\newnonterm e \varepsilon
\newnonterm{es}{\varepsilon}
\newnonterm{ec}{\hat\varepsilon}
\newnonterm r \rho
\newnonterm{ts}{\tau}
\newnonterm{tc}{\hat\tau}
\newnonterm s s
\newnonterm l l
\newnonterm B B
\newnonterm G \Gamma
\newnonterm{GC}{\hat\Gamma}
\newnonterm{vs} \varpi
\newnonterm{vc}{\hat\varpi}
\newnonterm f f
\newnonterm{gamma} \gamma

\newNTclass{gterm}
\newgterm x x
\newgterm v v
\newgterm n n
\newgterm p \pi

\newcommand{\figref}[1]{Figure \ref{fig:#1}}

\inputott{surface.ott}

\title{Compiling refinement types to dependent types}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

Type systems are a powerful tool for internal specification and verification of programs,
ensuring that programs exhibit certain desirable properties and do not exhibit undesirable ones.
In particular, dependent types \cite{FindSomethingForDTs} are powerful enough to encode pretty much arbitrary specifications
(going as far as encoding mathematics itself \cite{FindSomethingForCurryHoward}),
but this power comes at a cost: term finding (or, equivalently, proof finding) for dependent types is undecidable,
and so is type inference.
Or, practically speaking, the programmer must care about the details of the proofs,
even if the proofs lie within decidable theories
(like some flavours of arithmetic, sufficient for reasoning about array accesses and preventing out-of-bounds errors).
Even if the language supports some form of proof automation (such as proof by reflection, or Coq's tactics, or Idris' elaborator reflection),
this is still something that the programmer has to pay attention to.
Not only this consumes a part of human attention span (which is quite limited, compared to computers),
but we believe it also prevents widespread adoption of dependent types for mainstream programming.

A different point on the spectrum is represented by \emph{liquid types} \cite{LiquidTypes08},
where the expressive power is limited, and the type system only allows specifying properties that can be efficiently proven (or disproven) by an SMT solver.
Liquid types are a special case of a more general concept of \emph{refinement types} \cite{Constable87,Rushby98},
which are some base types endowed with a predicate, possibly dependent on other values, like \verb+{ v : Int | v >= 0 & v < len arr }+.
The logical implication on predicates
(or, equivalently, the set inclusion relation on the sets defined by predicates)
gives rise to a natural \emph{subtyping relation}:
for example, the aforementioned type is a subtype of \verb+{ v : Int | v >= -1 }+.
The core idea of liquid types then is to ensure that this subtype relation is efficiently decidable by an SMT solver.

Another useful property of refinement types is that they have a natural mental model:
indeed, it feels more approachable to treat \verb+{ v : Int | v >= 0 & v < len arr }+ as just an integer that is within some array's bounds
as opposed to the typical interpretation of
``a dependent pair consisting of an integer and a pair of proofs,
one for said integer being non-negative and another one for it being less than the length of \verb+arr+''.
Thus, refinement types are arguably more accessible and more usable in mainstream programming.

The downside is that efficiently decidable refinement types are less expressive, limiting what can be specified and proven.
Even though there are new developments pushing the envelope of what can be expressed with liquid types \cite{Reflection18},
the natural question is what would it take to combine full dependent types and refinement types in a single language.

Our work lays the theoretical foundation for positively answering this question, which, to the best of our knowledge, is a new contribution.
We consider a dependently typed language as some ``core'' language,
to which a ``surface'' language with just the refinement types is translated.
This provides several nice metatheoretical properties:
\begin{itemize}
  \item Dependent types are a well-studied and well-understood subject,
    so taking them as the translation target automatically provides us with the existing array of metatheorems.
  \item Since we require the solver to produce a proof of the subtyping relation,
    the result of the translation satisfies the de Bruijn criterion:
    even if the solver makes a mistake and produces an incorrect proof,
    this mistake would be caught by the type checker for the core language.
    We deem this a desirable property as it reduces the trusted code base size,
    since type checkers are typically simpler and have smaller ``error surface'' than SMT solvers,
    and, moreover, the core type checker has to be trusted anyway.
  \item We do not consider the peculiarities of a specific algorithm performing the subtyping check in the refinements language,
    introducing instead the abstraction of a \emph{subtyping oracle}.
    This allows plugging in not just SMT solvers but arbitrary decision procedures as long as they have certain properties we describe later.
  \item This approach transfers naturally to a fully dependently typed language such as Idris or Agda.
    For instance, an implementation might choose to reserve certain syntax (such as \verb+{ v : Ty | Predicate v }+)
    or a keyword to restrict type checking of a binding to the type theory with decidable proof finding.
\end{itemize}

Since verification of programs for safer software is the ultimate goal of all these type systems,
it seems important to have firm foundations and verify that the type systems themselves do make sense.
Thus, we also present mechanically verified proofs in Agda of all the key theorems about our surface language and its translation procedure.
This is, as far as we are aware, also a new contribution,
and no prior papers strived to mechanically verify the metatheoretical properties of refinement types to this extent.

\section{Overview}

\paragraph{Surface language.}
We define a surface language with refinement types inspired by \cite{LiquidTypes08,Reflection18}.
Broadly speaking, it is a simply-typed lambda calculus with several extensions,
refinements over the base types being the most important one.
The other extensions are:
\begin{itemize}
  \item dependent arrow types, to allow subsequent refinements refer preceding arguments,
  \item a limited form of algebraic data types (ADTs) and dependent pattern matching inspired by \cite{TAPLVariants,Eisenberg16},
    to illustrate reasoning about these widely used constructs, as well as generalizing path sensitivity \cite{FindSomethingForPathSensitivity}.
\end{itemize}

We also formulate and prove certain usual metatheorems about our surface language,
like the substitution lemmas on types and terms,
agreement lemmas between term well-typedness, type well-formedness and context well-formedness,
and some others.

One major difference between our surface language and the languages in \cite{LiquidTypes08,Reflection18} is that both parametric polymorphism as well as recursion are omitted.
This simplifies the formal model of the surface language considerably, while still giving meaningful results.

\paragraph{Core language.}
Our core language is a fully dependently typed language $\lambda C$ \cite{TTFPLambdaC}
(a pure type system \cite{Schmidt1994,Barendregt92} with two sorts)
extended with a unit type and a restricted form of ADTs.
We conjecture these extensions do not break the usual metatheoretical properties of the system,
and we explicitly prove progress and preservation theorems as a sanity check.

One plausible way to avoid the doubt in type safety of our core language stemming from the presence of the ADTs is to
encode them via the Boehm-Berarducci encoding \cite{Bohm85}.
Unfortunately, this does not work in our case,
as this encoding does not allow expressing the equality witness between the scrutinee of a pattern match and the patterns in each branch,
and it seemingly cannot be easily extended to account for this witness.

\paragraph{Subtyping oracle.}
The refinements in our surface language may contain arbitrary terms, including function application.
In general, this makes the subtyping check undecidable.
We sidestep this by abstracting the checker away and introducing a notion of an oracle.
The interface of said oracle boils down to a single function:
it takes a typing context and two refinements over a base type, and decides whether one is the subtype of the other.

Of course, the oracle can have false negatives, deciding that some refinements are not in the subtyping relationship, even if they are, in fact, related.
This does not affect the type safety of the surface language; the only requirement is the lack of false positives.

We also require certain properties regarding the oracle's behavior for the aforementioned surface language theorems to hold.
As an example, if the oracle yields a ``positive'' decision in some typing context, then it also must yield a ``positive'' decision in any bigger context,
extended with variable bindings irrelevant to the subtyping problem being considered.
All the required properties are mentioned in each individual metatheorem directly relying on them.

It's worth noting that the work on liquid types usually assumes the subtyping checks are done via an SMT solver, which can be seen to fit our oracle abstraction.

Moreover, although our surface language will turn out to be strongly normalizing, in practice most languages allow potentially non-terminating terms.
The oracle abstraction allows keeping the type checking decidable even in the presence of such terms at type level.
For instance, an oracle might just treat all functions as uninterpreted constants,
as is easily expressible in SMT solvers, and roughly as Idris type checker does \cite{Idris13}.

\paragraph{Combining refinement and dependent types.}
After formulating our surface and core languages, we define a translation from the former to the latter.
We also formulate two key theorems proving that the translation makes sense.
First, we prove that any well-typed surface language term has a translation to a well-typed core language term.
Then, we prove that the small-step operational semantics of the languages match and commute with the translation:
if a surface language term $[[ es ]]$ evaluates to some other term $[[ es' ]]$ in one step,
then the translation of $[[ es ]]$ to the core language evaluates into the translationof $[[ es' ]]$, although, perhaps, in several steps.

\section{Calculi definitions}

\subsection{Surface language}

The syntax of the surface language is presented in \figref{surface_syntax}.
The typing rules for the surface language are laid out in \figref{surface_typing}.
The (small-step) operational semantics for the surface language are presented in \figref{surface_opsem}.
That is largely the usual call-by-value evaluation model
with some extra rules for the ADTs and pattern matching.

To simplify the exposition and avoid dealing with shadowing,
all bindings in contexts are assumed to have different names.

\begin{figure}[ht]
  \footnotesize
  \begin{subfigure}{.6\textwidth}
    \nonterms{es,l,vs}
    \caption{Term level}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \nonterms{G,r,B,ts}
    \caption{Type level}
  \end{subfigure}
  \caption{Surface language syntax}
  \label{fig:surface_syntax}
\end{figure}

\begin{figure}[ht]
  \footnotesize
  \drules[TCTX]{$[[ G ok ]]$}{context well-formedness}{Empty,Bind}
  \drules[TWF]{$[[ G |- ts ]]$}{type well-formedness}{TrueRef,Base,Conj,Arr,ADT}
  \drules[T]{$[[ G |- es : ts ]]$}{term typing}{Unit,Var,Abs,App,Case,Con,Sub}
  \drules[ST]{$[[ G |- ts1 <: ts2 ]]$}{subtyping}{Base,Arr}
  \caption{Surface language typing}
  \label{fig:surface_typing}
\end{figure}

\begin{figure}[ht]
  \footnotesize
  \drules[E]{$[[ es1 ~~> es2 ]]$}{term evaluation}{AppL,AppR,AppAbs,ADT,CaseScrut,CaseMatch}
  \caption{Surface language small-step operational semantics}
  \label{fig:surface_opsem}
\end{figure}

We introduce a shorthand notation $\top$ denoting some tautology
which we use for trivial refinements that do not carry any extra information.
In particular, one good way to define $\top$ is
to state the equality of the sole inhabitant of $\Unit$ to itself:
$\top \triangleq \unit = \unit$.

\begin{remark}\label{remark:surface_base_types}
  The surface language might also have other base types (for example, $\Int$),
  elements of syntax (like numbers and operations on them)
  as well as typing rules relating those,
  but we omit them for the sake of brevity.
\end{remark}

We also have the following concessions:
\begin{itemize}
  \item The syntax only allows refinements on base types.
    Note that this does not restrict the expressive power of the language.
    For instance, a refinement on a function that we might otherwise wish to write as
    \[
      (x : \Int) \rightarrow \{ v : \Int \rightarrow \Int | v x = 0 \} \rightarrow \Int
    \]
    might instead be expressed via a dummy parameter:
    \[
      (x : \Int) \rightarrow (f : \Int \rightarrow \Int) \rightarrow \{ \_ : \Unit | f x = 0 \} \rightarrow \Int.
    \]
  \item Our algebraic data types only allow a single ``field'' in each constructor.
    One way to alleviate this is to introduce tuples,
    but doing so would complicate the exposition
    without any benefit for illustrating the main idea of this work.
\end{itemize}

The translation function $[|\ |]$ maps the typing context $\nonterm G$ to some logic
that the oracle can handle.
It might be defined as follows:
\begin{equation}
\begin{aligned}
  & [|\ \emptyset\ |]                    && \triangleq && \top																			           \\
  & [|\ [[ G , x : { v : B | r } ]] \ |] && \triangleq && [|\ [[ G ]] \ |] \land [|\ [[ [ v |-> x ] r ]] \ |]  \\
  & [|\ [[ r1 /\ r2 ]] \ |]              && \triangleq && [|\ [[ r1 ]] \ |] \land [|\ [[ r2 ]] \ |]            \\
  & [|\ [[ es_1 = es_2 ]] \ |]           && \triangleq && \text{oracle}([[ es_1 = es_2 ]])                     \\
\end{aligned}
\end{equation}

The last equation represents the assumption
that the oracle knows how to translate the atomic refinements.

%TODO define substitutions on the \nonterm r

In the subsequent text we also assume the oracle can decide the resulting proposition
(otherwise it might be left as a separate lemma to be filled out by the user in the core language, or rejected altogether).

\subsubsection{Properties}

We prove some of the usual metatheoretical properties of the proposed language.
We also provide machine-verified proofs of these theorems in the supplemental code,
with the corresponding proof terms referred in lemmas statements.

\begin{lemma}[Agreement I, \prooftermname{T-implies-TCTX}]\label{lma:term_wf_implies_ctx_wf}
  If $[[ G |- es : ts ]]$, then $[[ G ok ]]$.
\end{lemma}
\begin{proof}
  Straightforward induction on the derivation of $[[ G |- es : ts ]]$.
  \textsc{T-Unit} and \textsc{T-Var} carry the witness explicitly,
  and the other rules recurse on one of their premises.
\end{proof}

\begin{lemma}[Agreement II, \prooftermname{TWF-implies-TCTX}]\label{lma:type_wf_implies_ctx_wf}
  If $[[ G |- ts ]]$, then $[[G ok]]$.
\end{lemma}
\begin{proof}
  Straightforward induction on the derivation of $[[ G |- ts ]]$.
  \textsc{TWF-TrueRef} carries the witness explicitly,
  the case for \textsc{TWF-Base} uses \cref{lma:term_wf_implies_ctx_wf} on either of its premises,
  and the other rules recurse on one of their premises.
\end{proof}

For the following two lemmas,
assume $[[ G ]]$, $[[ G' ]]$ are contexts such that $[[ G ]] \subset [[ G' ]]$ and $[[ G' ok ]]$.
Then:
\begin{lemma}[Thinning on types, \prooftermname{twf-thinning}]\label{lma:thinning_twf}
  If $[[ G |- ts ]]$, then $[[ G' |- ts ]]$.
\end{lemma}
\begin{lemma}[Thinning on terms, \prooftermname{t-thinning}]\label{lma:thinning_t}
  If $[[ G |- es : ts ]]$, then $[[ G' |- es : ts ]]$.
\end{lemma}
These two lemmas are proven using mutual induction,
but since each lemma uses the other one (and itself) only on smaller derivation trees,
this is a well-founded proof.
\begin{proof}[Proof of \cref{lma:thinning_twf}]
	Mostly straightforward induction on the derivation of $[[ G |- ts ]]$.

  The case for the \textsc{TWF-Base} rule is worth paying special attention.
  Unlike other rules which only have type well-formedness premises,
  this one has premises requiring judgements about certain terms being well-typed,
  so this case uses the \cref{lma:thinning_t}.
\end{proof}
\begin{proof}[Proof of \cref{lma:thinning_t}]
  Again, induction on the derivation of $[[ G |- es : ts ]]$.
  \textsc{T-Abs}, \textsc{T-Case} and \textsc{T-Con} rules require
  thinning type well-formedness judgements, which is proven using \cref{lma:thinning_twf}.

  In addition to that, the \textsc{T-Sub} rule requires the oracle to satisfy the following condition:
  for any $[[ r1 ]], [[ r2 ]]$, if the oracle concludes $[[ [| G |] => A v. (r1 => r2) ]]$,
  then it must conclude $[[ [| G' |] => A v. (r1 => r2) ]]$.
\end{proof}

\begin{lemma}[Substitution on types, \prooftermname{sub-TWF}]\label{lma:type_substitution}
  Assume $[[ G, x : ts, GD |- ts' ]]$ and $[[ G |- es : ts ]]$, then $[[ G, [ x |-> es ] GD |- [ x |-> es ] ts' ]] $.
\end{lemma}
\begin{proof}
  Due to the presence of a subset of dependent types the proof is a bit bulky,
  using simultaneous induction on $\Delta$ and the derivation of $[[ G, x : ts, GD |- ts' ]]$.
  We refer the interested reader to the formal proof of theorem in the supplemental code for the specific details.
\end{proof}

\begin{lemma}[Agreement III, \prooftermname{T-implies-TWF}]\label{lma:term_wf_implies_type_wf}
  If $[[ G |- es : ts ]]$, then $[[ G |- ts ]]$.
\end{lemma}
\begin{proof}
  Straightforward induction on the derivation of $[[ G |- es : ts ]]$.
  The interesting case is \textsc{T-App},
  as it invokes the substitution lemma (\cref{lma:type_substitution}).
\end{proof}

\subsection{Core language}

The syntax of the core language is presented in \figref{core_syntax},
the core $\lambda C$ typing rules are defined in \figref{core_typing}
and the rules for our extensions are defined in \figref{core_typing_exts}.
Note that the \textsc{CT-ADTCase} rule uses $\equiv$ denoting the equality
which is a derived form defined later.

As a general convention,
core entities corresponding to the surface language will be denoted by the same symbol, but with a $\widehat{\text{hat}}$.
For instance, core typing contexts are denoted as $[[ GC ]]$ and core expressions are denoted as $[[ ec ]]$.
Although there is no separate syntactic notion of a type,
$[[ tc ]]$ is used a a metavariable where a type (an inhabitant of $\star$) is expected.
The $\_$ placeholder is used where a binding name is expected but the binding is not used in the corresponding subterm,
as in $[[ \ _ : ec. unitc ]]$ or, for a more general example, $[[ Pi _ : tc1. tc2 ]]$.

In addition to that, we will somewhat frivolously use $\hat B$
to denote the core language analogue of a surface language base type $B$,
which exists for $\Unit$ ($\hat B$ then being $\Unitc$)
and which we assume to exist for any other surface language base type
mentioned in \cref{remark:surface_base_types}.
We also assume that $\hat B$ has type $\star$ without any extra premises.

Even though we believe that our core language enjoys the strong normalization property even with our extensions,
we still explicitly define the evaluation rules (\figref{core_opsem}).
Firstly, this allows fixing a specific evaluation strategy mirroring the one for the surface language,
simplifying the subsequent proofs of some key theorems.
Secondly, the rule for \textsc{CE-CaseMatch},
as well as our flavour of pattern matching in general,
deserves some special attention.
\todo[inline]{Pay said attention.}

\begin{figure}[ht]
  \footnotesize
  \nonterms{ec,s,vc}
  \caption{Core language syntax}
  \label{fig:core_syntax}
\end{figure}

\begin{figure}[ht]
  \footnotesize
  \drules[CT]{$[[ GC |- ec1 : ec2 ]]$}{core typing}{Sort,Var,Weaken,Form,App,Abs,Conv}
  \caption{$\lambda C$ typing rules}
  \label{fig:core_typing}
\end{figure}
\begin{figure}[ht]
  \footnotesize
  \drules[CT]{$[[ GC |- ec1 : ec2 ]]$}{core typing, extensions}{UnitType,UnitTerm,ADTForm,ADTCon,ADTCase}
  \caption{Typing rules for our extensions to $\lambda C$}
  \label{fig:core_typing_exts}
\end{figure}

\begin{figure}[ht]
  \footnotesize
  \drules[CE]{$[[ ec1 ~~> ec2 ]]$}{term evaluation}{AppL,AppR,AppAbs,ADT,CaseScrut,CaseMatch}
  \caption{Core language small-step operational semantics}
  \label{fig:core_opsem}
\end{figure}

\newcommand{\dast}{\ **\ }

\paragraph{Derived forms.}
We also establish some derived forms in the core language to simplify the subsequent exposition,
and state some straightforward facts that will be useful later:
\begin{itemize}
  \item Simplified variable typing rule which we call \textsc{CT-VarW}:
    \[ \ottdruleCTXXVarW{} \]
    This rule unfolds into a single \textsc{CT-Var} defining $x$,
    followed by a sequence of \textsc{CT-Weaken}
    to ``move'' the $x$ into the right position in the final $[[ GC ]]$.
  \item Non-dependent function type:
    \[
      [[ tc1 -> tc2 ]] \triangleq [[ Pi _ : tc1. tc2 ]].
    \]
  \item Dependent pair type, or $\Sigma$-type,
    with the first component being a value $x$ of type $[[tc]]$
    and the second component being of type $P x$, where $P$ is usually of type $[[tc]] \rightarrow \star$:
    \[
      [[ (x : tc ** Pred x) ]] \triangleq [[ Pi alpha : sst. (Pi x : tc. Pred x -> alpha) -> alpha ]],
    \]
    with the corresponding constructor (we overload the syntax here to resemble Idris \cite{Idris13}):
    \[
			[[ (x ** p) ]] \triangleq [[ \ alpha : sst. \ f : (Pi x' : tc. Pred x' -> alpha). f x p ]]
		\]
    and projections
    \[
      \text{fst} \triangleq [[ \ sigma : (x : tc ** Pred x). sigma tc (\ x' : tc. \ _ : Pred x'. x') ]],
    \]
    \[
      \text{snd} \triangleq [[ \ sigma : (x : tc ** Pred x). sigma tc (\ x' : tc. \ p : Pred x'. p) ]].
    \]
    It is easy to see from the formation rule that
    \begin{lemma}\label{lma:dep_pair_typing}
      If $[[ tc ]]$ is typeable in $[[ GC ]]$
      and $[[ Pred x ]]$ is typeable in $[[ GC, x : tc ]]$,
      then $[[ GC |- (x : tc ** Pred x ) : sst ]]$.
    \end{lemma}

    We also introduce a helper $[[seconddp]]$ that will be useful later.
    It takes a function and uses it to map over the second component of a dependent pair,
    or, in types:
    \[
      [[ empty |- seconddp : Pi ec , ec1 , ec2 : sst. (Pi x : ec . ec1 -> ec2) -> (x : ec ** ec1) -> (x : ec ** ec2) ]].
    \]
    It has an unsurprising but perhaps cumbersome definition:
    \[
      [[ \ ec , ec1 , ec2 : sst. \ f : (Pi x : ec . ec1 -> ec2). \ x' : (x : ec ** ec1). x' (x : ec ** ec2) (\ x0 : ec. \ x1 : ec1. (x0 ** f x0 x1)) ]].
    \]

  \item Non-dependent pair type:
    \[
      (\nonterm{tc}_1, \nonterm{tc}_2) \triangleq \{ x : \nonterm{tc}_1\ **\ (\lambda \_ : \nonterm{tc}_1. \nonterm{tc}_2) x \}.
    \]
    As a direct consequence of the previous lemma we obtain
    \begin{lemma}\label{lma:non_dep_pair_typing}
      If $\nonterm{tc}_1, \nonterm{tc}_2$ are typeable in $\nonterm{GC}$,
      then $(\nonterm{tc}_1, \nonterm{tc}_2)$ is typeable in $\nonterm{GC}$ and has type $\star$.
    \end{lemma}
  \item Equality of expressions $\nonterm{ec}_1, \nonterm{ec}_2$ of type $\nonterm{tc}$:
    \[
      \nonterm{ec}_1 \equiv \nonterm{ec}_2
        \triangleq
        \Pi P : \nonterm{tc} \rightarrow \star. (P \nonterm{ec}_1 \rightarrow P \nonterm{ec}_2, P \nonterm{ec}_2 \rightarrow P \nonterm{ec}_1).
    \]
    The reader might recognize this as the Leibniz equality \cite{FindSomethingForLeibnizEq}.

    The corresponding introduction rule $\eqrefl x$ of type $x \equiv x$
    stating that $x$ of type $\nonterm{tc}$ is equal to itself
    is then
    \[
      \eqrefl x \triangleq \lambda P : \nonterm{tc} \rightarrow \star. (\lambda p : P x. p, \lambda p : P x. p).
    \]

    Again, it can be seen that
    \begin{lemma}\label{lma:equality_typing}
      If $\nonterm{ec}_1, \nonterm{ec}_2$ are well-typed in a context $\nonterm{GC}$,
      then $\nonterm{GC} \vdash \nonterm{ec}_1 \equiv \nonterm{ec}_2 : \star$.
    \end{lemma}
\end{itemize}

\section{Translation}

\newcommand{\tranty}{\mu_\tau}
\newcommand{\tranterm}{\mu_\varepsilon}
\newcommand{\transub}{\mu_{<:}}

\newcommand{\Tranctx}{\mu_{\vdash\Gamma}}
\newcommand{\Tranty}{\mu_{\vdash\tau}}
\newcommand{\Tranterm}{\mu_{\vdash\varepsilon}}
\newcommand{\Transub}{\mu_{\vdash <:}}

%FIXME ugly, but whatever
\renewcommand{\ottdrulename}[1]{}

The surface language ultimately gets translated into the core language.
What really matters for practical purposes is the translation of the terms, since
terms, not types or contexts, will eventually be compiled down to some executable code.
On the other hand, it seems desirable to ensure that the terms
that are well-typed in the surface language
are accepted by the core type checker after the translation,
providing some level of guarantee that the translation ``makes sense''.

As the oracle plays a crucial role in type checking the surface language,
it is natural to expect that it will also be used during any such translation,
for instance, to produce core language proofs that VCs hold.
The only trace of the oracle's work is in the typing derivations
(namely, in the subtyping relation check and the \textsc{T-Sub} typing rule),
so we choose to do the translation not on \emph{terms},
but rather on \emph{well-typedness derivations} for the terms.
This way the oracle has a chance to enrich the original surface language term
with proofs and witnesses of whatever it decided.

Thus the ultimate goal of this section is to define a function $\Tranterm$
taking a typing derivation in the surface language
and producing a typing derivation in the core language.
As part of its duty
this function also needs to translate the types and contexts
encountered in the source derivation,
so we also define two helper functions.
$\Tranty$ takes a surface derivation of a \emph{type} well-formedness
and produces the corresponding core derivation.
$\Tranctx$ takes a surface derivation of a \emph{context} well-formedness
and produces a core language context.

We also define two helper functions, $\tranty$ and $\tranterm$,
invoking $\Tranty$ and $\Tranterm$ and extracting just the type and term component respectively.

All in all, we define $\Tranctx$, $\Tranty$ and $\Tranterm$
with ``metatypes''
\begin{align*}
  \Tranctx  & : [[ G ok ]]         \longmapsto [[ GC ]],             \\
  \Tranty   & : [[ G |- ts ]]      \longmapsto [[ GC |- tc : sst ]], \\
  \Tranterm & : [[ G |- es : ts ]] \longmapsto [[ GC |- ec : tc ]].
\end{align*}
Our helper functions $\tranty$ and $\tranterm$ are then defined to return the $\nonterm{tc}$ and $\nonterm{ec}$
from the respective right-hand sides.
The apparent asymmetry of $\Tranctx$, which does not produce a derivation as others do,
is due to our core language not having a separate notion of a context well-formedness.

Note that, due to \cref{lma:type_wf_implies_ctx_wf} we might assume that
there is a derivation of $[[ G ok ]]$ around whenever there is a derivation of $[[ G |- ts ]]$.
Indeed, the proof of \cref{lma:type_wf_implies_ctx_wf} is constructive,
so there is a function taking a witness of $[[ G |- ts ]]$ and producing $[[ G ok ]]$.
Analogously, \cref{lma:term_wf_implies_type_wf} and \cref{lma:term_wf_implies_ctx_wf}
allow us to use a derivation of $[[ G |- ts ]]$ and $[[ G ok ]]$ respectively
whenever there is a derivation of $[[ G |- es : ts ]]$.

\paragraph{Contexts.}
As a warm-up,
we start with defining $\Tranctx$,
which is the simplest of the three.
It merely maps the types of the bindings in a (well-formed) context
from the surface language into the core language.

Since $\Tranctx$ is defined on the derivations of $[[ G ok ]]$,
consider the last rule used in a derivation:
\begin{itemize}
  \item \textsc{TCTX-Empty}.
    \begin{align*}
      & \ottdruleTCTXXXEmpty{} \longmapsto \emptyset.
    \end{align*}
    The base case is trivial: the empty context is mapped to the empty context.
  \item \textsc{TCTX-Bind}.
    \begin{align*}
      & \ottdruleTCTXXXBind{}
          \longmapsto
        [[ mudG ( G ok ) , x : muT ( G |- ts ) ]].
    \end{align*}

    We recurse on the prefix $[[ G ]]$,
    which is admissible due to the $[[ G ok ]]$ premise,
    and we use $\tranty$ to get the translated type of $\gterm x$,
    which is also admissible due to the other premise.
\end{itemize}

\paragraph{Types.}
Next, we define $\Tranty$. In short:
\begin{itemize}
  \item (dependent) arrow types are translated to the corresponding $\Pi$-types,
  \item refined types are translated to the corresponding $\Sigma$-types,
  \item atomic refinements, being propositions about surface language terms equality,
    are translated to propositions stating the equality of translated core language terms,
  \item conjunctions of several (atomic) refinements are represented as tuples of the corresponding atomic translations.
\end{itemize}

More formally, consider the last rule used in the derivation of $[[ G |- ts ]]$:
\begin{itemize}
  \item \textsc{TWF-TrueRef}.
    \begin{align*}
      & \ottdruleTWFXXTrueRef{} \longmapsto \ottdruleTWFTargetXXTrueRef{}, \\
      \text{where}~&[[ GC = mudG(G ok) ]].
    \end{align*}

    By \cref{lma:non_dep_pair_typing}
    $[[ (BC, unitc equal unitc) ]]$ has type $\star$ in $[[ GC ]]$
    if $[[ BC ]]$ and $[[ unitc equal unitc ]]$ are well-typed in $[[ GC ]]$,
    which, in turn, is easily checked.
    Thus, there exists a derivation of $[[ GC |- (BC, unitc equal unitc) : sst ]]$,
    and it is effectively constructible, although omitted for brevity.
  \item \textsc{TWF-Base}.
    \begin{align*}
      & \ottdruleTWFXXBase{} \longmapsto \ottdruleTWFTargetXXBase{}, \\
      \text{where}~&[[ GC = mudG(G ok) ]], \\
                   &[[ ec_i = muE(G, v : { v_1 : B | top } |- es_i : { v2 : B' | r_i }) ]].
    \end{align*}

    To see that this is well-defined, first note that
    that $[[ BC ]]$ has type $\star$ by assumption on $[[ BC ]]$.
    Then, $[[ ec_1 ]]$ and $[[ ec_2 ]]$ are well-typed in the context $[[ mudG (G, v : { v_1 : B | top } ok) ]]$ according to the premises,
    and this context coincides with the one used to type check $[[ \v : BC. ec_1 equal ec_2 ]]$.
    Hence both components of the dependent pair are well-typed, so by \cref{lma:dep_pair_typing} the dependent pair itself has type $\star$.

    \todo[inline]{
      The ${ v : B | \top }$ in the $\nonterm G '$ desugars to a dependent pair, yet we use $\lambda v : \hat B$ — note the naked base type.
      Figure out how to best fix this.
    }

    Just as in the previous case, the extra derivations and premises corresponding to the considerations above
    are omitted and denoted by $\cdots$.
  \item \textsc{TWF-Conj}.
    \begin{align*}
      & \ottdruleTWFXXConj{} \longmapsto \ottdruleTWFTargetXXConj{}, \\
      \text{where}~&[[ GC = mudG(G ok)]],                                         \\
                   &P_1 \text{~s.t.~} [[ ( v : BC ** \ v : BC . Pred1 v) = muT ( G |- { v : B | r1 }) ]],  \\
                   &P_2 \text{~s.t.~} [[ ( v : BC ** \ v : BC . Pred2 v) = muT ( G |- { v : B | r2 }) ]].
    \end{align*}

    The proof of the right-hand side being well-defined goes similarly to the cases considered previously,
    but with an extra step.
    Namely, note that the last two ``patterns'' on the left-hand side are irrefutable:
    it can be seen by direct inspection and inductive reasoning that $\tranty$ always yields a pair
    for a derivation of the form $[[ G |- { v : B | r }]]$.
    Thus we can take the predicate components $P_1, P_2$ out from the corresponding dependent pairs
    and repack them obtaining $[[ \ v : BC . (Pred1 v, Pred2 v) ]]$.
  \item \textsc{TWF-Arr}.
    \begin{align*}
      & \ottdruleTWFXXArr{} \longmapsto \ottdruleTWFTargetXXArr{}, \\
      \text{where}~&[[ GC = mudG(G ok) ]],                \\
                   &[[ tc1 = muT(G |- ts1) ]],            \\
                   &[[ tc2 = muT(G , x : ts1 |- ts2) ]].
    \end{align*}

    The rule \textsc{CT-Form} is used on the right.
    This is admissible,
    since $[[ mudT(G |- ts1) ]]$ produces a derivation of $[[ GC |- tc1 : sst ]]$,
    and similarly there is a derivation of $[[ GC , x : tc1 |- tc2 : sst ]]$
    resulting from the other invocation of $[[ mudT ]]$.
  \item \textsc{TWF-ADT}.
    \begin{align*}
      & \ottdruleTWFXXADT{} \longmapsto \ottdruleTWFTargetXXADT{}, \\
      \text{where}~&[[ GC = mudG(G ok) ]],                \\
                   &[[ tci = muT(G |- tsi) ]].
    \end{align*}

    It can be seen that the right-hand side is well-defined
    by an argument similar to ones for the previous clauses.
\end{itemize}

\paragraph{Terms.}
Finally we define $\Tranterm$.
We consider the term typing rule at the root of the derivation tree for $[[ G |- es : ts ]]$.

\begin{itemize}
  \item \textsc{T-Unit}.
    \begin{align*}
      & \ottdruleTXXUnit{} \longmapsto \ottdruleCTXXUnitTerm{}, \\
      \text{where}~&\nonterm{GC} = \Tranctx(\nonterm G \ctxok).
    \end{align*}
    Any other base type $B$ is treated similarly.

  \item \textsc{T-Var}.
    \begin{align*}
      & \ottdruleTXXVar{} \longmapsto \ottdruleTTargetXXVarW{}, \\
      \text{where}~&[[ GC = mudG(G ok) ]],       \\
                   &[[ tc = muT(G |- ts) ]].
    \end{align*}

    To show that the right-hand side is well-defined,
    first note that $x : \nonterm{tc} \in \nonterm{GC}$ holds.
    Indeed, by definition of $\Tranctx$,
    if $\nonterm G$ contains a binding for the name $x$,
    then $\nonterm{GC}$ also contains a binding for that name,
    and its type is precisely $[[ muT ( G |- ts ) ]]$.
    Secondly, $[[ G |- x : ts ]]$ implies $[[ G |- ts ]]$ by \cref{lma:term_wf_implies_type_wf},
    and $[[ mudT(G |- ts) ]]$ provides a judgement of the form $[[ GC |- tc : s ]]$.

    All in all, this shows that all the premises
    necessary for the (derived) core typing rule \textsc{CT-VarW}
    hold.

  \item \textsc{T-Abs}.
    \begin{align*}
      & \ottdruleTXXAbs{} \longmapsto \ottdruleTTargetXXAbs{}, \\
      \text{where}~&[[ GC = mudG(G ok) ]],       \\
                   &[[ tc1 = muT(G |- ts1) ]], \\
                   &[[ tc2 = muT(G , x : ts1 |- ts2) ]], \\
                   &[[ ec = muE(G , x : ts1 |- es : ts2) ]].
    \end{align*}

    The right-hand side here is well-formed.
    Firstly, $[[ mudT ( G |- (x : ts1) -> ts2 ) ]]$
    produces a derivation tree of the form $[[ GC |- Pi x : tc1. tc2 : sst ]]$,
    as can be seen by inspecting the definition of $[[ mudT ]]$ for \textsc{TWF-Arr},
    which is the only rule having $[[ G |- (x : ts1) -> ts2 ]]$ in the conclusion.
    Then, $[[ mudE ( G , x : ts1 |- es : ts2 ) ]]$
    produces a derivation of $[[ GC , x : tc1 |- ec : tc2 ]]$.
    These two derivations can then be used as premises for \textsc{CT-Abs},
    which is invoked on the right-hand side.

  \item \textsc{T-App}.
    \begin{align*}
      & \ottdruleTXXApp{} \longmapsto \ottdruleTTargetXXApp{}, \\
      \text{where}~&[[ GC = mudG(G ok) ]],       \\
                   &[[ tc2 = muT(G |- ts2) ]],       \\
                   &[[ ec1 = muE(G |- es1 : (x : ts1) -> ts2) ]], \\
                   &[[ ec2 = muE(G |- es2 : ts1) ]].
    \end{align*}

    Admissibility of the right-hand side follows from a similar argument.

  \item \textsc{T-Con}.
    \begin{align*}
      & \ottdruleTXXCon{} \longmapsto \ottdruleTTargetXXCon{}, \\
      \text{where}~&[[ GC = mudG(G ok) ]],       \\
                   &[[ tci = muT(G |- tsi) ]], \\
                   &[[ ec = muE(G |- es : tsj) ]].
    \end{align*}

    Well-formedness of the right-hand side can be shown analogously to the previous clauses.
    One extra thing to note is that,
    although $[[ G |- tsi ]]$ are not present directly in the premises of \textsc{T-Con},
    they are implied by the $[[ G |- < </ li : tsi // i /> > ]]$ premise and thus can be used.

  \item \textsc{T-Case}.
    \begin{align*}
      & \ottdruleTXXCase{} \\
      \longmapsto \quad & \ottdruleTTargetXXCase{}, \\
      \text{where}~&[[ GC = mudG(G ok) ]],       \\
                   &[[ tc' = muT(G |- ts') ]],       \\
                   &[[ ec = muE(G |- es : ts) ]],    \\
                   &[[ eci = muE(G |- esi : ts) ]],  \\
                   &[[ tci' ]] = [[ (_ : Unitc ** ec equal < li = xi > as tc) ]].
    \end{align*}

    This part of the translation is perhaps one of the most involved,
    and the main complication stems from the following.
    In \textsc{T-Case}, each (surface) $[[esi]]$ is being type checked in the context
    where $x$ (the witness of the $\mathbf{case}$ branch taken)
    has type $[[ { _ : Unit | es = < li = xi > as ts } ]]$.
    After $[[esi]]$ gets translated into the (core) term $[[eci]]$,
    the latter is type checked in the context where $\hat x$
    has the dependent pair type $[[ (_ : Unitc ** ec equal < li = xi > as tc) ]]$.
    On the other hand, the $\mathbf{proof} \pi_i$ part of the $\mathbf{case}$ branch
    provides the proof $\pi_i$ typed as $[[ ec equal < li = xi > as tc ]]$,
    so it needs to be packed into the dependent pair type that $[[eci]]$ ``expects''.

    The lambda abstraction/application pair on the right-hand side of the translation
    is responsible for this extra packing.
    All the typing derivations showing that this is well-formed follow trivially
    from the typing rules, but are again omitted and replaced by $\dots$ for the sake of brevity.

  \item \textsc{T-Sub}.
    This is the only part of the translation that uses the oracle,
    and hence cannot be done as straightforwardly as the other ones.

    Our goal is to translate a typing derivation ending with \textsc{T-Sub}:
    \[
      \ottdruleTXXSub{}.
    \]

    Note that, for languages with non-trivial subtyping, uniqueness of typing does not hold,
    but it holds for the CoC modulo $\beta$-conversion
    (which is an equivalence relation unlike any non-trivial subtyping relation).
    Thus, intuitively,
    there is no general way to prescribe both the translation of $[[ ts ]]$ and $[[ ts' ]]$
    to the translation of $[[ es ]]$ simultaneously.
    Instead, we produce another term that carries along the witness of the subtyping relation.

    \todo[inline]{Should we give the intuition for producing a new term
      based on the uniqueness of typing-like considerations or
      based on VC proof witness stuff?}

    We now define yet another translation function, $\Transub$,
    mapping the derivations of $[[ G |- ts <: ts' ]]$
    into core language terms $[[gamma]]$ of type $[[ muT(G |- ts) -> muT(G |- ts') ]]$
    along with derivations of their well-typedness.
    Informally, forgetting about the distinction between surface and core types,
    a subtyping derivation is turned into a function $[[gamma]]$
    transforming any term of the subtype into a term of the supertype.

    The helper function $\transub$ is defined similarly to other helpers,
    forgetting about the derivation and yielding just the final term.

    So, consider the last (subtyping) rule in the derivation of $[[ G |- ts <: ts' ]]$:
    \begin{itemize}
      \item \textsc{ST-Base}.
        In this case $[[ts]]$ and $[[ts']]$ are
        $[[ { v : B | r1 } ]]$ and $[[ { v : B | r2 } ]]$
        respectively for some $[[B]]$, $[[r1]]$ and $[[r2]]$.
        What is needed is a function of type
        $[[ muT(G |- { v : B | r1 }) -> muT(G |- { v : B | r1 }) ]]$.

        \begin{align*}
          & \ottdruleSTXXBase{} \longmapsto \ottdruleSTTargetXXBase{}, \\
          \text{where}~&[[ GC = mudG(G ok) ]],       \\
                       &[[ tc1 ]] \text{~s.t.~} [[ ( v : BC ** tc1) = muT ( G |- { v : B | r1 }) ]],  \\
                       &[[ tc2 ]] \text{~s.t.~} [[ ( v : BC ** tc2) = muT ( G |- { v : B | r2 }) ]],  \\
                       &[[ ec = muE(G |- es : ts) ]].
        \end{align*}

        This is the base case for the subtyping relation,
        and this is the only case where the oracle is involved,
        so we have no choice but require the oracle to produce a core language term $\sigma$ such that
        $[[ GC |- sigma : Pi v : BC . tc1 -> tc2 ]]$,
        where $[[ GC = mudG(G ok) ]]$, and $[[tc1]]$, $[[tc2]]$ correspond to the translations
        of the refinement terms $[[r1]]$, $[[r2]]$ respectively.

        Intuitively (or, perhaps, Curry-Howard-ly),
        this $[[ sigma ]]$ witnesses the implication of the refinements $[[ A v. r1 => r2 ]]$,
        which is isomorphic to the function type $[[ Pi v : BC. tc1 -> tc2 ]]$.

        Then, this $[[ sigma ]]$ can be combined with $[[ seconddp ]]$,
        producing a function mapping $[[ (v : BC ** tc1) ]]$ onto $[[ (v : BC ** tc2 ) ]]$~---
        precisely what is needed in case of \textsc{ST-Base}.

      \item \textsc{ST-Arr}.
        \begin{align*}
          & \ottdruleSTXXArr{} \\
          \longmapsto \quad & \ottdruleSTTargetXXArr{}, \\
          \text{where}~&[[ GC = mudG(G ok) ]],       \\
                       &[[ gamma1 = muS(G |- ts1' <: ts1) ]],  \\
                       &[[ gamma2 = muS(G , x : ts1' |- ts2 <: ts2') ]],  \\
                       &[[ ec = muE(G |- es : ts) ]].
        \end{align*}

        Intuitively, there is a function of type $[[ (x : ts1) -> ts2 ]]$,
        and we need to produce a function of type $[[ (x : ts1') -> ts2' ]]$
        given $[[ G |- ts1' <: ts1 ]]$ and $[[ G , x : ts1' |- ts2 <: ts2' ]]$.
        Then, the first premise is used (via $[[gamma1]]$) to turn $[[x]]$ of type $[[ts1']]$
        into a value of type $[[ts1]]$ that the existing function can consume,
        producing a result of type $[[ ts2 ]]$,
        which is then similarly turned (by $[[gamma2]]$) into a value of type $[[ ts2' ]]$.

        This intuition is then formalized analogously to the previous clause.
    \end{itemize}

    With $\Transub$ defined, translation of the \textsc{T-Sub} rule is straightforward:
    \begin{align*}
      & \ottdruleTXXSub{} \longmapsto \ottdruleTTargetXXSub{}, \\
      \text{where}~&[[ GC = mudG(G ok) ]],       \\
                   &[[ ec = muE(G |- es : ts) ]], \\
                   &[[ gamma = muS(G |- ts <: ts') ]].
    \end{align*}
\end{itemize}

Having defined these functions, we formulate a simple but useful lemma:
\begin{lemma}[Translation totality]
  $\Tranctx$, $\Tranty$ and $\Tranterm$ are total.
\end{lemma}
\begin{proof}
  Firstly, each function is defined for all possible inputs.
  Then, each function recurses on structurally smaller inputs,
  and, since derivation trees are finite,
  this implies each function terminates for any input.
\end{proof}

\subsection{Correctness}

The correctness of the translation hinges on two key theorems.
The first one claims that a well-typed program in surface language remains well-typed after the translation to the core language.
The second one claims that a well-typed surface language program evaluates to ``the same'' value as the translated one.

\begin{theorem}[Well-typedness preservation]
  For any well-typed surface language term $[[ es ]]$,
  the result of the translation via $[[ muE ]]$ is ``equivalently'' typeable in the core language.
\end{theorem}
\begin{proof}
  This follows directly from the definition of $\tranterm$ via $\Tranterm$,
  the latter being total and mapping onto typing derivations in the core language.
\end{proof}

\begin{theorem}[Evaluation equivalence]
  If $[[ es ]]$ is a well-typed surface language term and $[[ es ~~> es' ]]$,
  then the translation of $[[ es ]]$ evaluates to the translation of $[[ es' ]]$ (perhaps, in several steps).
\end{theorem}
\begin{proof}
\end{proof}

\section{Practical consequences}

Having refinement types in a dependently typed language is useful in multiple ways.

Firstly, it indeed lowers the cognitive load and the amount of (keyboard) typing that the programmer has to do.
A significant class of runtime errors, such as out-of-bounds array access, division by zero, and other examples motivating refinement types,
is eliminatable without any explicit proofs from the programmer.

Then, the language combining these typing systems might allow both ``refinemently typed'' and dependently typed terms in a single module,
while also allowing to use the former in the latter and vice versa.
This enables the programmer to mostly write code with refinement types,
only resorting to the full power of dependent types when refinements are not enough.

Moreover, since types are first-class in a dependently typed language,
the refinements can also be treated as first-class, passing them around, returning from functions and so on.

\section{Future work}

As we mentioned in the overview, our surface language lacks two components critical for any practically useful language:
namely, polymorphism and recursion.
Extending our language with polymorphism is straightforward, while adding general recursion is non-trivial,
since it needs to be translated to some dependently typed language, and those are typically strongly normalizing.
Investigating this is an interesting and useful direction for future work,
and one might think of the following approaches (or a combination thereof):
\begin{itemize}
  \item Require the oracle to produce a proof of termination of each recursive function.
    The oracle then can use approaches analogous to the termination checkers \cite{abel1998foetus} in Idris or Agda,
    or the heuristics outlined in the works on Liquid Haskell \cite{Vazou16}, or any other algorithm.
  \item Mark the recursive functions as non-terminating, propagate this property to the callers,
    and treat non-terminating functions as uninterpreted during type checking.
    It is worth noting that, in particular,
    Idris allows functions that are not provably terminating and treats them in a similar way \cite{Idris13}.

    Indeed, there are two reasons why termination is so desirable for a dependently typed language:
    \begin{itemize}
      \item Termination ensures decidability of type checking, which relies on $\beta$-equivalence of terms.
        By treating functions that are not provably terminating as not having any reduction behaviour \emph{during type checking},
        we ensure type checking remains decidable, even if at a cost of rejecting some programs that otherwise would be well-typed.
        Moreover, we deem this cost insignificant for the intended uses of languages with refinement typing.
      \item Termination is crucial to the soundness of the logic that the language expresses.
        Propagating the property of ``being non-terminating'' to the callers ensures that
        $\bot$ can only be proven in contexts that are explicitly marked (by the type checker or the programmer) as non-total,
        and hence the programmer knows they have not really proven anything.
        But this is already the existing state of affairs: it is surely possible to have a term of type $\bot$, say, in Idris or Agda;
        it is just the termination checker that needs to be disabled for such a term.
    \end{itemize}
\end{itemize}

Another, practical direction for future work lies in reflecting the entities from the dependently typed language into the oracle world.
For example, suppose the programmer has defined their own \verb+Nat+ type, among with the operations on it and proofs of their properties.
What would it take to teach the oracle that type, its semantics and its operations?
Investigating how to do this in a sound and feasible way seems important to make any language combining refinement types and dependent types practically useful.

\section{Conclusions}

\bibliographystyle{babunsrt-lf}
\bibliography{biblio}

\end{document}
